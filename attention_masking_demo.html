<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Attention Masking Demo</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background: #f5f5f5;
        }
        .container {
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        .step {
            margin: 30px 0;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 8px;
            border-left: 4px solid #007bff;
        }
        .matrix {
            display: inline-block;
            margin: 10px;
            padding: 15px;
            background: white;
            border-radius: 5px;
            border: 1px solid #ddd;
        }
        .matrix-title {
            font-weight: bold;
            margin-bottom: 10px;
            color: #333;
        }
        table {
            border-collapse: collapse;
            margin: 10px 0;
        }
        td, th {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: center;
            min-width: 60px;
        }
        th {
            background: #e9ecef;
            font-weight: bold;
        }
        .masked {
            background: #ffebee !important;
            color: #c62828;
            font-weight: bold;
        }
        .allowed {
            background: #e8f5e8 !important;
            color: #2e7d32;
            font-weight: bold;
        }
        .tokens {
            display: flex;
            gap: 20px;
            margin: 20px 0;
        }
        .token {
            padding: 10px 15px;
            background: #e9ecef;
            border-radius: 5px;
            border: 2px solid #dee2e6;
            font-weight: bold;
        }
        .warning {
            background: #fff3cd;
            padding: 15px;
            border-radius: 5px;
            margin: 10px 0;
            border: 1px solid #ffeaa7;
        }
        .insight {
            background: #d1ecf1;
            padding: 15px;
            border-radius: 5px;
            margin: 10px 0;
            border: 1px solid #bee5eb;
        }
        .arrow {
            font-size: 24px;
            color: #007bff;
            margin: 0 10px;
        }
        .code-block {
            background: #f8f9fa;
            padding: 15px;
            border-radius: 5px;
            font-family: 'Courier New', monospace;
            border: 1px solid #dee2e6;
            margin: 10px 0;
        }
        .future-token {
            background: #ffcdd2;
            border: 2px solid #f44336;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üîí Attention Masking: Preventing Future Peek</h1>
        
        <div class="step">
            <h2>The Problem: Looking into the Future</h2>
            <div class="tokens">
                <div class="token">Token 1: "The"</div>
                <div class="token">Token 2: "cat"</div>
                <div class="token future-token">Token 3: "sat" (FUTURE!)</div>
            </div>
            <div class="warning">
                <p><strong>‚ö†Ô∏è Problem:</strong> When generating "cat", the model shouldn't know that "sat" comes next!</p>
                <p>This is called the <strong>causal constraint</strong> - each token can only attend to previous tokens and itself.</p>
            </div>
        </div>

        <div class="step">
            <h2>Step 1: Raw Attention Scores (Before Masking)</h2>
            <p>From our previous example, we computed:</p>
            
            <div class="matrix" style="margin: 20px auto;">
                <div class="matrix-title">Attention Scores (Q @ K<sup>T</sup>)</div>
                <table>
                    <tr><th>Query ‚Üì Key ‚Üí</th><th>Key1 ("The")</th><th>Key2 ("cat")</th><th>Key3 ("sat")</th></tr>
                    <tr><th>Query1 ("The")</th><td class="allowed">0.56</td><td class="masked">0.51</td><td class="masked">0.31</td></tr>
                    <tr><th>Query2 ("cat")</th><td class="allowed">0.42</td><td class="allowed">0.68</td><td class="masked">0.51</td></tr>
                    <tr><th>Query3 ("sat")</th><td class="allowed">0.20</td><td class="allowed">0.52</td><td class="allowed">0.85</td></tr>
                </table>
            </div>
            
            <div class="insight">
                <p><strong>üö® Red cells are ILLEGAL!</strong> They represent attention to future tokens:</p>
                <ul>
                    <li>"The" shouldn't attend to "cat" or "sat" (future tokens)</li>
                    <li>"cat" shouldn't attend to "sat" (future token)</li>
                    <li>"sat" can attend to all previous tokens (legal)</li>
                </ul>
            </div>
        </div>

        <div class="step">
            <h2>Step 2: Create the Mask</h2>
            <p>We create a boolean mask where <code>True</code> means "BLOCK this attention":</p>
            
            <div class="code-block">
# Upper triangular mask (excluding diagonal)
mask = torch.triu(torch.ones(3, 3), diagonal=1).bool()
            </div>
            
            <div class="matrix" style="margin: 20px auto;">
                <div class="matrix-title">Boolean Mask</div>
                <table>
                    <tr><th>Position ‚Üì Position ‚Üí</th><th>Pos 1</th><th>Pos 2</th><th>Pos 3</th></tr>
                    <tr><th>Pos 1</th><td>False</td><td class="masked">True</td><td class="masked">True</td></tr>
                    <tr><th>Pos 2</th><td>False</td><td>False</td><td class="masked">True</td></tr>
                    <tr><th>Pos 3</th><td>False</td><td>False</td><td>False</td></tr>
                </table>
            </div>
            
            <div class="insight">
                <p><strong>Mask Logic:</strong></p>
                <ul>
                    <li><strong>True (red):</strong> Block this attention (future tokens)</li>
                    <li><strong>False (white):</strong> Allow this attention (current & past tokens)</li>
                </ul>
            </div>
        </div>

        <div class="step">
            <h2>Step 3: Apply the Mask</h2>
            <div class="code-block">
# Apply mask: replace True positions with -‚àû
attn_scores.masked_fill(mask_bool, -torch.inf)
            </div>
            
            <div style="display: flex; align-items: center; justify-content: center;">
                <div class="matrix">
                    <div class="matrix-title">Before Masking</div>
                    <table>
                        <tr><th></th><th>K1</th><th>K2</th><th>K3</th></tr>
                        <tr><th>Q1</th><td>0.56</td><td>0.51</td><td>0.31</td></tr>
                        <tr><th>Q2</th><td>0.42</td><td>0.68</td><td>0.51</td></tr>
                        <tr><th>Q3</th><td>0.20</td><td>0.52</td><td>0.85</td></tr>
                    </table>
                </div>
                
                <div class="arrow">‚Üí</div>
                
                <div class="matrix">
                    <div class="matrix-title">After Masking</div>
                    <table>
                        <tr><th></th><th>K1</th><th>K2</th><th>K3</th></tr>
                        <tr><th>Q1</th><td class="allowed">0.56</td><td class="masked">-‚àû</td><td class="masked">-‚àû</td></tr>
                        <tr><th>Q2</th><td class="allowed">0.42</td><td class="allowed">0.68</td><td class="masked">-‚àû</td></tr>
                        <tr><th>Q3</th><td class="allowed">0.20</td><td class="allowed">0.52</td><td class="allowed">0.85</td></tr>
                    </table>
                </div>
            </div>
        </div>

        <div class="step">
            <h2>Step 4: Why -‚àû (Negative Infinity)?</h2>
            <p>After masking, we apply <strong>softmax</strong> to get attention weights:</p>
            
            <div class="code-block">
attn_weights = torch.softmax(attn_scores, dim=-1)
            </div>
            
            <div class="matrix" style="margin: 20px auto;">
                <div class="matrix-title">After Softmax</div>
                <table>
                    <tr><th></th><th>K1</th><th>K2</th><th>K3</th></tr>
                    <tr><th>Q1</th><td class="allowed">1.00</td><td class="masked">0.00</td><td class="masked">0.00</td></tr>
                    <tr><th>Q2</th><td class="allowed">0.44</td><td class="allowed">0.56</td><td class="masked">0.00</td></tr>
                    <tr><th>Q3</th><td class="allowed">0.16</td><td class="allowed">0.21</td><td class="allowed">0.63</td></tr>
                </table>
            </div>
            
            <div class="insight">
                <p><strong>Magic of -‚àû:</strong> When you apply softmax to -‚àû, it becomes 0!</p>
                <p>This completely eliminates attention to future tokens. üéØ</p>
            </div>
        </div>

        <div class="step">
            <h2>Step 5: The Result</h2>
            <div class="insight">
                <h3>üéâ Mission Accomplished!</h3>
                <ul>
                    <li><strong>Token 1 ("The"):</strong> Can only attend to itself (100%)</li>
                    <li><strong>Token 2 ("cat"):</strong> Attends to "The" (44%) and itself (56%)</li>
                    <li><strong>Token 3 ("sat"):</strong> Attends to all previous tokens: "The" (16%), "cat" (21%), itself (63%)</li>
                </ul>
                
                <p><strong>No future peeking allowed!</strong> Each token only uses information from the past and present.</p>
            </div>
        </div>

        <div class="step">
            <h2>üîë Code Breakdown</h2>
            <div class="code-block">
# 1. Transpose to get right shape for matrix multiplication
keys = keys.transpose(1,2)
queries = queries.transpose(1,2)
values = values.transpose(1,2)

# 2. Compute raw attention scores
attn_scores = queries @ keys.transpose(2,3)

# 3. Get the mask for current sequence length
mask_bool = self.mask.bool()[:num_tokens, :num_tokens]

# 4. Apply mask: -‚àû where mask is True
attn_scores.masked_fill(mask_bool, -torch.inf)
            </div>
            
            <div class="warning">
                <p><strong>Why transpose(2,3) for keys?</strong> Because we need the last two dimensions to be <code>(head_dim, num_tokens)</code> for the dot product to work correctly!</p>
            </div>
        </div>
    </div>
</body>
</html>